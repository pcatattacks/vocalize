{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython, numpy as np, scipy as sp, matplotlib.pyplot as plt, matplotlib, sklearn, librosa\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline\n",
    "\n",
    "import audio_utils\n",
    "import vamp\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"VAMP_PATH\"] = \"./melodia_plugins/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melodic_frequency_filepath(filepath):\n",
    "    audio, sr = librosa.load(filepath, mono=True)\n",
    "    data = vamp.collect(audio, sr, \"mtg-melodia:melodia\")\n",
    "    hop, melody = data['vector']\n",
    "    return melody\n",
    "\n",
    "def melodic_frequency(audio, sr):\n",
    "    data = vamp.collect(audio, sr, \"mtg-melodia:melodia\")\n",
    "    hop, melody = data['vector']\n",
    "    return melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A clearer option is to get rid of the negative values before plotting\n",
    "def plot_melody(melody):\n",
    "    timestamps = 8 * 128/44100.0 + np.arange(len(melody)) * (128/44100.0)\n",
    "    melody_pos = melody[:]\n",
    "    melody_pos[melody<=0] = None\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(timestamps, melody_pos)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAMP_CONSTANT = (128/44100.0)\n",
    "\n",
    "def timestamp_to_index(timestamp):\n",
    "    index = (timestamp - (8 * VAMP_CONSTANT)) / VAMP_CONSTANT\n",
    "    return int(index)\n",
    "\n",
    "def find_interval_freq(start_time, end_time, melody):\n",
    "    start_index = timestamp_to_index(start_time)\n",
    "    end_index = timestamp_to_index(end_time)\n",
    "    \n",
    "    relevant_frequencies = melody[start_index:end_index]\n",
    "    \n",
    "    freq = find_freq(relevant_frequencies)\n",
    "    \n",
    "    return freq\n",
    "\n",
    "def find_freq(melody):\n",
    "    voiced_frequencies = np.array([ freq for freq in melody if freq > 0 ])\n",
    "    freq = np.median(voiced_frequencies)\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_play(y, sr, label=''):\n",
    "    print(label)\n",
    "    IPython.display.display(Audio(y, rate=sr))\n",
    "    audio_utils.plot_audio(y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import subprocess\n",
    "\n",
    "SPEECH_FILEPATH = 'temp/tts_dump.wav'\n",
    "TEXT_FILEPATH = 'temp/text.txt'\n",
    "SCRIPT_FILEPATH = \"./text_gen.sh\"\n",
    "\n",
    "# def tts(text):\n",
    "#     \"\"\"\n",
    "#         returns list of AudioSegments corresponding to each word in text\n",
    "#     \"\"\"\n",
    "#     tokens = text.split(' ')\n",
    "    \n",
    "#     audio_chunks = [tts_word(token) for token in tokens]\n",
    "\n",
    "#     return audio_chunks\n",
    "\n",
    "def tts_word(text, sr):\n",
    "\n",
    "    with open(TEXT_FILEPATH, 'w+') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    p = subprocess.call(SCRIPT_FILEPATH, shell=True)\n",
    "\n",
    "    y = AudioSegment.from_wav(SPEECH_FILEPATH)\n",
    "\n",
    "    x = y.set_frame_rate(sr)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub.silence import split_on_silence\n",
    "\n",
    "def get_syllable_signals(text, sr):\n",
    "    params = {'min_silence_len': 40, \n",
    "              'silence_thresh': -18, \n",
    "              'keep_silence': 10\n",
    "             }\n",
    "    audio_words = tts_word(text, sr)\n",
    "    audio_words = audio_words.set_frame_rate(sr)\n",
    "\n",
    "    audio_chunks = split_on_silence(audio_words, \n",
    "                                    min_silence_len=params['min_silence_len'], \n",
    "                                    silence_thresh=params['silence_thresh'], \n",
    "                                    keep_silence=params['keep_silence'])\n",
    "\n",
    "    all_segments = [np.array(chunk.get_array_of_samples(), dtype=float) for chunk in audio_chunks]\n",
    "\n",
    "    return all_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def onset_intervals(y, sr):\n",
    "    onset_times = librosa.onset.onset_detect(y=y, sr=sr, units='time')\n",
    "    \n",
    "    final_time = len(y)/ sr\n",
    "    \n",
    "    onset_intervals = [onset_interval(i, onset, onset_times, final_time) for i, onset in enumerate(onset_times)]\n",
    "    \n",
    "    return onset_intervals\n",
    "\n",
    "def onset_interval(i, onset_start, onset_times, last_timestamp):\n",
    "        if i+1 >= len(onset_times):\n",
    "            return (onset_start, last_timestamp)\n",
    "        \n",
    "        else:\n",
    "            return (onset_start, onset_times[i+1])\n",
    "    \n",
    "def onset_frequencies(y, sr):\n",
    "    intervals = onset_intervals(y, sr)\n",
    "    y_melody = melodic_frequency(y, sr)\n",
    "    y_freqs = [find_interval_freq(start, end, y_melody) for start, end in intervals]\n",
    "    return intervals, y_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_difference(fn, f0):\n",
    "    n_steps = np.round(np.log2(float(fn)/f0) * 12)\n",
    "    return n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def autotune(signals, fn, sr):\n",
    "    \n",
    "    shifted_signals = [pitch_correct(y=y, fn=f, sr=sr) for y, f in zip(signals, fn)]\n",
    "    \n",
    "    return shifted_signals\n",
    "\n",
    "def pitch_correct(y, fn, sr):\n",
    "    f0 = 150\n",
    "    \n",
    "    n_steps = pitch_difference(fn=fn, f0=f0)\n",
    "\n",
    "    shifted_y = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=n_steps)\n",
    "\n",
    "    return shifted_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_signals(signals, intervals, sr):\n",
    "    stretchy = [stretch_signal(y=y, interval=interval, sr=sr) for y, interval in zip(signals, intervals)]\n",
    "    return stretchy\n",
    "\n",
    "def stretch_signal(y, interval, sr):\n",
    "    \n",
    "        note_duration = interval[1] - interval[0]\n",
    "        y_duration = librosa.get_duration(y=y, sr=sr)\n",
    "        rate = y_duration/note_duration\n",
    "        stretched_y = librosa.effects.time_stretch(y, rate=rate)\n",
    "        return stretched_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2e(filename, text, sr):\n",
    "    song, sr = librosa.load(filename, sr=sr)\n",
    "    \n",
    "    intervals, frequencies = onset_frequencies(song, sr)\n",
    "    \n",
    "    syllables = get_syllable_signals(text, sr)\n",
    "\n",
    "    pitch_corrected_syllables = autotune(syllables, fn=frequencies, sr=sr)\n",
    "    \n",
    "    time_corrected_syllables = stretch_signals(pitch_corrected_syllables, intervals, sr=sr)\n",
    "    \n",
    "    tuned_voice_signal = np.concatenate(time_corrected_syllables)\n",
    "    \n",
    "    song_with_voice = overlay_signals(song, tuned_voice_signal, sr)\n",
    "    \n",
    "    return song_with_voice\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_signals(song_signal, vocal_signal, sr):\n",
    "    \n",
    "    first_onset_sample = librosa.onset.onset_detect(y=song_signal, sr=sr, units='samples')[0]\n",
    "    \n",
    "    song_trimmed = song_signal[first_onset_sample:]\n",
    "\n",
    "    min_length = min(len(song_trimmed), len(vocal_signal))\n",
    "    \n",
    "    both = librosa.util.normalize(song_trimmed[:min_length]) + librosa.util.normalize(vocal_signal[:min_length])\n",
    "    \n",
    "    return both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 44100\n",
    "text = 'miss congeniality was average'\n",
    "y = e2e(filename='audio/hbd_snip.wav', text=text, sr=sr)\n",
    "plot_and_play(y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "syllables = get_syllable_signals('abdicate', sr=44100)\n",
    "end = time.time()\n",
    "\n",
    "one_word = end-start\n",
    "print('one word', one_word)\n",
    "\n",
    "word_count = 1000\n",
    "one_trial = one_word * word_count\n",
    "print('one trial', one_trial/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_data(filename, n):\n",
    "    with open(filename, 'r') as r:\n",
    "        doc = yaml.load(r)\n",
    "        step = len(doc)/n\n",
    "        doc2 = doc[::step]\n",
    "        words = np.array([obj['word'] for obj in doc2])\n",
    "        counts = np.array([obj['count'] for obj in doc2])\n",
    "        return words, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(filename, data):\n",
    "    np.save(filename, data, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ'):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end='\\r')\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(signals, counts, silence_thresh, silence_len):\n",
    "\n",
    "    records = np.zeros(shape=(silence_thresh.shape[0], silence_len.shape[0]))\n",
    "\n",
    "    n_trials = len(silence_thresh)*len(silence_len)\n",
    "    \n",
    "    for i, st in enumerate(silence_thresh):\n",
    "        for j, sl in enumerate(silence_len):\n",
    "            results = run_trial(silence_thresh=st, silence_len=sl, signals=signals)\n",
    "            accuracy = evaluate(results, counts)\n",
    "            records[i,j] = accuracy\n",
    "            current_trial = i*len(silence_len) + j + 1\n",
    "            printProgressBar(current_trial, n_trials)\n",
    "            \n",
    "                   \n",
    "    return records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(silence_thresh, silence_len, signal):\n",
    "    syllables = split_on_silence(signal, \n",
    "                                    min_silence_len=silence_len, \n",
    "                                    silence_thresh=silence_thresh)\n",
    "    return len(syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(silence_thresh, silence_len, signals):\n",
    "    \n",
    "    counts = np.array([syllable_count(silence_thresh, silence_len, signal) for signal in signals])\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(counts, answers):\n",
    "    correct = np.sum(counts == answers)\n",
    "    accuracy = float(correct) / len(answers)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, counts = load_data('syllables/syllables-pruned.yaml', 500)\n",
    "sr = 44100\n",
    "signals = [tts_word(word, sr) for word in words]\n",
    "silence_thresh = np.arange(\n",
    "    start=-16, \n",
    "    stop=-22,\n",
    "    step=-1)\n",
    "# -10 to -50 \n",
    "# best = -18\n",
    "silence_len = np.arange(\n",
    "    start=12, \n",
    "    stop=72,\n",
    "    step=2)\n",
    "# 0 to 400\n",
    "# best = 100\n",
    "\n",
    "results = run_experiment(signals, counts, silence_thresh, silence_len)\n",
    "plot_results(results, silence_thresh, silence_len)\n",
    "filename = 'syllables/results_' + str(silence_thresh[0]) + '_' +str(silence_thresh[-1]) + '_' + str(silence_len[0]) + '_'+str(silence_len[-1])\n",
    "write_data(filename, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = np.zeros(shape=(silence_thresh.shape[0], silence_len.shape[0]))\n",
    "\n",
    "for i, st in enumerate(silence_thresh):\n",
    "    for j, sl in enumerate(silence_len):\n",
    "        result = results[i*len(silence_len)+j]['accuracy']\n",
    "        records[i,j]=result\n",
    "plot_results(records, silence_thresh, silence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results, silence_thresh, silence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argwhere(results == np.amax(results))\n",
    "best_indices = best.tolist()\n",
    "best_combos = [(silence_thresh[st], silence_len[sl]) for st, sl in best_indices]\n",
    "print('accuracy', np.amax(results))\n",
    "for c in best_combos: print(c)\n",
    "\n",
    "# {'silence_thresh': -18, 'silence_len': 40, 'accuracy': 0.9705882352941176}\n",
    "# {'silence_thresh': -18, 'silence_len': 48, 'accuracy': 0.9705882352941176}\n",
    "# {'silence_thresh': -18, 'silence_len': 54, 'accuracy': 0.9705882352941176}\n",
    "# {'silence_thresh': -19, 'silence_len': 40, 'accuracy': 0.9705882352941176}\n",
    "# {'silence_thresh': -19, 'silence_len': 44, 'accuracy': 0.9705882352941176}\n",
    "# {'silence_thresh': -19, 'silence_len': 46, 'accuracy': 0.9705882352941176}\n",
    "# {'silence_thresh': -19, 'silence_len': 48, 'accuracy': 0.9705882352941176}\n",
    "# {'silence_thresh': -19, 'silence_len': 50, 'accuracy': 0.9705882352941176}\n",
    "# {'silence_thresh': -19, 'silence_len': 52, 'accuracy': 0.9705882352941176}\n",
    "# {'silence_thresh': -19, 'silence_len': 54, 'accuracy': 0.9705882352941176}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(arr, silence_thresh, silence_len, x_axis, y_axis, cbarlabel=\"\", title=\"\"):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    ax = plt.gca()\n",
    "    im = ax.imshow(arr, cmap='hot', interpolation='nearest')\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, shrink=0.25)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "\n",
    "    ax.set_xticks(np.arange(arr.shape[1], step=5))\n",
    "    ax.set_yticks(np.arange(arr.shape[0], step=2))\n",
    "\n",
    "    ax.set_xticklabels(silence_len[::5])\n",
    "    ax.set_yticklabels(silence_thresh[::2])\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    plt.savefig('foo.pdf')\n",
    "    plt.show()\n",
    "# -15 -30\n",
    "# 10 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "silence_thresh = np.arange(\n",
    "    start=-15, \n",
    "    stop=-30,\n",
    "    step=-1)\n",
    "# -10 to -50 \n",
    "# best = -18\n",
    "silence_len = np.arange(\n",
    "    start=10, \n",
    "    stop=170,\n",
    "    step=2)\n",
    "\n",
    "xlabel='Minimum Silence Length (ms)'\n",
    "ylabel='Silence Threshold (dbFS)'\n",
    "\n",
    "records = np.load('syllables/results_100words_-15_-30dbFS_10_170ms.npy')\n",
    "\n",
    "plot_results(records, silence_thresh, silence_len, xlabel, ylabel, cbarlabel='Accuracy', title=\"PyDub's split_on_silence Syllable Partitioning Accuracy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
