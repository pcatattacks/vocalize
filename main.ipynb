{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython, numpy as np, scipy as sp, matplotlib.pyplot as plt, matplotlib, sklearn, librosa\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline\n",
    "\n",
    "import audio_utils\n",
    "import vamp\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"VAMP_PATH\"] = \"./melodia_plugins/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melodic_frequency_filepath(filepath):\n",
    "    \"\"\"\n",
    "    Deprecated.\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(filepath, mono=True)\n",
    "    data = vamp.collect(audio, sr, \"mtg-melodia:melodia\")\n",
    "    hop, melody = data['vector']\n",
    "    return melody\n",
    "\n",
    "def melodic_frequency(audio, sr):\n",
    "    \"\"\"\n",
    "    Uses Melodia to extract the main, monophonic frequencies played at each instance of time for a given signal.\n",
    "    \"\"\"\n",
    "    data = vamp.collect(audio, sr, \"mtg-melodia:melodia\")\n",
    "    hop, melody = data['vector']\n",
    "    return melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A clearer option is to get rid of the negative values before plotting\n",
    "def plot_melody(melody):\n",
    "    timestamps = 8 * 128/44100.0 + np.arange(len(melody)) * (128/44100.0)\n",
    "    melody_pos = melody[:]\n",
    "    melody_pos[melody<=0] = None\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(timestamps, melody_pos)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAMP_CONSTANT = (128/44100.0)\n",
    "\n",
    "def timestamp_to_index(timestamp):\n",
    "    \"\"\"\n",
    "    Used to index into output of melodic_frequency function.\n",
    "    \"\"\"\n",
    "    index = (timestamp - (8 * VAMP_CONSTANT)) / VAMP_CONSTANT\n",
    "    return int(index)\n",
    "\n",
    "def index_to_timestamp(idx):\n",
    "    \"\"\"\n",
    "    Inverses the operation of the timestamp_to_index function.\n",
    "    \n",
    "    Won't be the exact timestamp because of the int() operation in timestamp_to_idx.\n",
    "    \"\"\"\n",
    "    timestamp = (idx * VAMP_CONSTANT) + (8 * VAMP_CONSTANT)\n",
    "    return timestamp\n",
    "\n",
    "def find_interval_freq(start_time, end_time, melody):\n",
    "    \"\"\"\n",
    "    Given a start, end time and a melody found by Melodia, finds the fundamental frequency being played at the interval.\n",
    "    \n",
    "    Takes the median frequency being played in the given interval.\n",
    "    \"\"\"\n",
    "    start_index = timestamp_to_index(start_time)\n",
    "    end_index = timestamp_to_index(end_time)\n",
    "    \n",
    "    relevant_frequencies = melody[start_index:end_index]\n",
    "    \n",
    "    freq = find_freq(relevant_frequencies)\n",
    "    \n",
    "    return freq\n",
    "\n",
    "def find_freq(melody):\n",
    "    voiced_frequencies = np.array([ freq for freq in melody if freq > 0 ])\n",
    "    freq = np.median(voiced_frequencies)\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_play(y, sr, label=''):\n",
    "    print(label)\n",
    "    IPython.display.display(Audio(y, rate=sr))\n",
    "    audio_utils.plot_audio(y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pranav/dev/vocalize/venv/lib/python2.7/site-packages/pydub/utils.py:165: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import subprocess\n",
    "\n",
    "SPEECH_FILEPATH = 'temp/tts_dump.wav'\n",
    "TEXT_FILEPATH = 'temp/text.txt'\n",
    "SCRIPT_FILEPATH = \"./text_gen.sh\"\n",
    "\n",
    "def tts(text):\n",
    "    \"\"\"\n",
    "    Deprecated.\n",
    "    \"\"\"\n",
    "    tokens = text.split(' ')\n",
    "    \n",
    "    audio_chunks = [tts_word(token) for token in tokens]\n",
    "\n",
    "    return audio_chunks\n",
    "\n",
    "def tts_word(text):\n",
    "    \"\"\"\n",
    "    Returns list of AudioSegments corresponding to each word in text.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(TEXT_FILEPATH, 'w+') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    p = subprocess.call(SCRIPT_FILEPATH, shell=True)\n",
    "\n",
    "    y = AudioSegment.from_wav(SPEECH_FILEPATH)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub.silence import split_on_silence\n",
    "\n",
    "def get_syllable_signals(text, sr):\n",
    "    \"\"\"\n",
    "    Returns list of signals corresponding to syllables in words of the provided text.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'min_silence_len': 40, \n",
    "        'silence_thresh': -18, \n",
    "        'keep_silence': 10\n",
    "    }\n",
    "    audio_words = tts_word(text)\n",
    "    audio_words = audio_words.set_frame_rate(sr)\n",
    "\n",
    "    audio_chunks = split_on_silence(audio_words, \n",
    "                                    min_silence_len=params['min_silence_len'], \n",
    "                                    silence_thresh=params['silence_thresh'], \n",
    "                                    keep_silence=params['keep_silence'])\n",
    "\n",
    "    all_segments = [np.array(chunk.get_array_of_samples(), dtype=float) for chunk in audio_chunks]\n",
    "\n",
    "    return all_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def onset_intervals(y, y_melody, sr):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples of (onset_start, onset_end). Onset end is essentially when the note ends.\n",
    "    \"\"\"\n",
    "    onset_times = librosa.onset.onset_detect(y=y, sr=sr, units='time')\n",
    "    \n",
    "    final_time = len(y)/ sr\n",
    "    \n",
    "    onset_intervals = [onset_interval(i, onset, onset_times, final_time, y_melody) for i, onset in enumerate(onset_times)]\n",
    "    \n",
    "    return onset_intervals\n",
    "\n",
    "def onset_interval(i, onset_start, onset_times, last_timestamp, y_melody):\n",
    "    \"\"\"\n",
    "    \"Helper function for above ^\"\n",
    "    \"\"\"\n",
    "    # converting to indices so we may index into y_melody (sourced from melodia)\n",
    "    start_idx = timestamp_to_index(onset_start)\n",
    "    if i+1 >= len(onset_times):\n",
    "        end_idx = len(y_melody)\n",
    "    else:\n",
    "        end_idx = timestamp_to_index(min(onset_times[i+1], last_timestamp))\n",
    "\n",
    "    # iterating over interval between this and the next onset\n",
    "    for idx in range(start_idx, end_idx):\n",
    "        # if no melody playing, the note must have ended\n",
    "        if y_melody[idx] < 0.0:\n",
    "            onset_end = index_to_timestamp(idx)\n",
    "            return onset_start, onset_end\n",
    "\n",
    "    # means that there was no silence in the melody during the interval duration\n",
    "    return (onset_start, onset_times[i+1])\n",
    "    \n",
    "def onset_frequencies(y, sr):\n",
    "    \"\"\"\n",
    "    Given a signal and sample rate:\n",
    "    \n",
    "     - returns list of intervals and corresponding frequencies as a tuple of 2 lists.\n",
    "    \"\"\"\n",
    "    y_melody = melodic_frequency(y, sr)\n",
    "    intervals = onset_intervals(y, y_melody, sr)\n",
    "#     print(\" printing output of melodic_frequency function:\") # debug\n",
    "#     print(y_melody) # debug\n",
    "    y_freqs = [find_interval_freq(start, end, y_melody) for start, end in intervals]\n",
    "    return intervals, y_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_difference(fn, f0):\n",
    "    n_steps = np.round(np.log2(float(fn)/f0) * 12)\n",
    "    return n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def autotune(signals, fn, sr):\n",
    "    \"\"\"\n",
    "    Given a list of signals and corresponding frequencies, autotunes each signal to corresponding frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    shifted_signals = [pitch_correct(y=y, fn=f, sr=sr) for y, f in zip(signals, fn)]\n",
    "    \n",
    "    return shifted_signals\n",
    "\n",
    "def pitch_correct(y, fn, sr):\n",
    "    \"\"\"\n",
    "    Given a signal and a frequency, autotunes the signal to the frequency.\n",
    "    \"\"\"\n",
    "    f0 = 150\n",
    "    \n",
    "    n_steps = pitch_difference(fn=fn, f0=f0)\n",
    "\n",
    "    shifted_y = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=n_steps)\n",
    "\n",
    "    return shifted_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_signals(signals, intervals, sr):\n",
    "    stretchy = [stretch_signal(y=y, interval=interval, sr=sr) for y, interval in zip(signals, intervals)]\n",
    "    return stretchy\n",
    "\n",
    "def stretch_signal(y, interval, sr):\n",
    "    \"\"\"\n",
    "    Stretches a signal to sound for the duration of the interval.\n",
    "    \"\"\"\n",
    "    note_duration = interval[1] - interval[0]\n",
    "    y_duration = librosa.get_duration(y=y, sr=sr)\n",
    "    rate = y_duration/note_duration\n",
    "    stretched_y = librosa.effects.time_stretch(y, rate=rate)\n",
    "    return stretched_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2e(filename, text, sr):\n",
    "    song, sr = librosa.load(filename, sr=sr)\n",
    "    \n",
    "    intervals, frequencies = onset_frequencies(song, sr)\n",
    "    \n",
    "    syllables = get_syllable_signals(text, sr)\n",
    "\n",
    "    pitch_corrected_syllables = autotune(syllables, fn=frequencies, sr=sr)\n",
    "    \n",
    "    time_corrected_syllables = stretch_signals(pitch_corrected_syllables, intervals, sr=sr)\n",
    "    \n",
    "    tuned_voice_signal = np.concatenate(time_corrected_syllables)\n",
    "    \n",
    "    song_with_voice = overlay_signals(song, tuned_voice_signal, sr)\n",
    "    \n",
    "    return song_with_voice\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_signals(song_signal, vocal_signal, sr):\n",
    "    \n",
    "    first_onset_sample = librosa.onset.onset_detect(y=song_signal, sr=sr, units='samples')[0]\n",
    "    \n",
    "    song_trimmed = song_signal[first_onset_sample:]\n",
    "\n",
    "    min_length = min(len(song_trimmed), len(vocal_signal))\n",
    "    \n",
    "    both = librosa.util.normalize(song_trimmed[:min_length]) + librosa.util.normalize(vocal_signal[:min_length])\n",
    "    \n",
    "    return both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr = 44100\n",
    "# text = 'happy birthday to you'\n",
    "# y = e2e(filename='audio/hbd_snip.wav', text=text, sr=sr)\n",
    "# plot_and_play(y, sr)\n",
    "\n",
    "sr = 44100\n",
    "text = 'Trumpet'\n",
    "y = e2e(filename='audio/trumpet.wav', text=text, sr=sr)\n",
    "plot_and_play(y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
