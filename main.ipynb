{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython, numpy as np, scipy as sp, matplotlib.pyplot as plt, matplotlib, sklearn, librosa\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline\n",
    "\n",
    "import audio_utils\n",
    "import vamp\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"VAMP_PATH\"] = \"./melodia_plugins/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melodic_frequency_filepath(filepath):\n",
    "    audio, sr = librosa.load(filepath, mono=True)\n",
    "    data = vamp.collect(audio, sr, \"mtg-melodia:melodia\")\n",
    "    hop, melody = data['vector']\n",
    "    return melody\n",
    "\n",
    "def melodic_frequency(audio, sr):\n",
    "    data = vamp.collect(audio, sr, \"mtg-melodia:melodia\")\n",
    "    hop, melody = data['vector']\n",
    "    return melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A clearer option is to get rid of the negative values before plotting\n",
    "def plot_melody(melody):\n",
    "    timestamps = 8 * 128/44100.0 + np.arange(len(melody)) * (128/44100.0)\n",
    "    melody_pos = melody[:]\n",
    "    melody_pos[melody<=0] = None\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(timestamps, melody_pos)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOP = 128.0\n",
    "\n",
    "def sample_to_index(sample):\n",
    "    index = sample/HOP - 8\n",
    "    \n",
    "    return int(index)\n",
    "\n",
    "def find_interval_freq(start_sample, end_sample, melody):\n",
    "    start_index = sample_to_index(start_sample)\n",
    "    end_index = sample_to_index(end_sample)\n",
    "    \n",
    "    relevant_frequencies = melody[start_index:end_index]\n",
    "    \n",
    "    freq = find_freq(relevant_frequencies)\n",
    "    \n",
    "    return freq\n",
    "\n",
    "def find_freq(melody):\n",
    "    voiced_frequencies = melody[melody > 0]\n",
    "    freq = np.median(voiced_frequencies)\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_play(y, sr, label=''):\n",
    "    print(label)\n",
    "    IPython.display.display(Audio(y, rate=sr))\n",
    "    audio_utils.plot_audio(y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import subprocess\n",
    "\n",
    "SPEECH_FILEPATH = 'temp/tts_dump.wav'\n",
    "TEXT_FILEPATH = 'temp/text.txt'\n",
    "SCRIPT_FILEPATH = \"./text_gen.sh\"\n",
    "\n",
    "def tts(text):\n",
    "    \"\"\"\n",
    "        returns list of AudioSegments corresponding to each word in text\n",
    "    \"\"\"\n",
    "    tokens = text.split(' ')\n",
    "    \n",
    "    audio_chunks = [tts_word(token) for token in tokens]\n",
    "\n",
    "    return audio_chunks\n",
    "\n",
    "def tts_word(text, sr):\n",
    "\n",
    "    with open(TEXT_FILEPATH, 'w+') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    p = subprocess.call(SCRIPT_FILEPATH, shell=True)\n",
    "\n",
    "    y = AudioSegment.from_wav(SPEECH_FILEPATH)\n",
    "    \n",
    "    y = y.set_frame_rate(sr)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub.silence import split_on_silence\n",
    "\n",
    "def get_syllable_signals(text, sr):\n",
    "    params = {'min_silence_len': 40, \n",
    "              'silence_thresh': -18, \n",
    "              'keep_silence': 100\n",
    "             }\n",
    "    \n",
    "    audio_words = tts_word(text, sr)\n",
    "\n",
    "    audio_chunks = split_on_silence(audio_words, \n",
    "                                    min_silence_len=params['min_silence_len'], \n",
    "                                    silence_thresh=params['silence_thresh'], \n",
    "                                    keep_silence=params['keep_silence'])\n",
    "\n",
    "    all_segments = [np.array(chunk.get_array_of_samples(), dtype=float) for chunk in audio_chunks]\n",
    "\n",
    "    return all_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def louisa():\n",
    "    print(onset_times.shape)\n",
    "    print(\"onsets with clicks\")\n",
    "    clicks = librosa.clicks(times=onset_times, sr=sr, length=len(y))\n",
    "    plot_and_play(clicks + y, sr)\n",
    "    plt.figure()\n",
    "    plt.plot(times, onset_env, alpha=0.8, label='Onset strength')\n",
    "    plt.vlines(times[peaks], 0, onset_env.max(), color='r', alpha=0.8, label='Selected peaks')\n",
    "    plt.legend(frameon=True, framealpha=0.8)\n",
    "    plt.axis('tight')\n",
    "    plt.tight_layout()\n",
    "    print(peaks)\n",
    "    print(onset_times.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import madmom\n",
    "\n",
    "def onset_intervals(y, sr):\n",
    "    proc = madmom.features.onsets.OnsetPeakPickingProcessor()\n",
    "    act = madmom.features.onsets.RNNOnsetProcessor()(y)\n",
    "    onset_times = proc(act)\n",
    "\n",
    "    onset_samples = librosa.core.time_to_samples(onset_times, sr=sr)\n",
    "    starts = onset_samples[:len(onset_samples)-1]\n",
    "    ends = np.append(onset_samples[1:], [len(y)])\n",
    "    intervals = zip(starts, ends)\n",
    "    return intervals\n",
    "    \n",
    "def onset_frequencies(y, song, sr):\n",
    "    intervals = onset_intervals(y, sr)\n",
    "    y_melody = melodic_frequency(y, sr)\n",
    "    y_freqs = [find_interval_freq(start, end, y_melody) for start, end in intervals]\n",
    "    print(y_freqs)\n",
    "    intervals, y_freqs = zip(*[(i, f) for i, f in zip(intervals, y_freqs) if not np.isnan(f)])\n",
    "\n",
    "    return intervals, y_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_difference(fn, f0):\n",
    "    n_steps = np.round(np.log2(float(fn)/f0) * 12)\n",
    "    return n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_voice_frequency():\n",
    "    filepath = 'espeak-data/voices/robot'\n",
    "    with open(filepath) as f:\n",
    "        lines = [line.split(' ') for line in f.readlines()]\n",
    "        line = [line for line in lines if line and line[0] == 'pitch'][0]\n",
    "        f0 = int(line[1])\n",
    "        return f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def autotune(signals, fn, sr):\n",
    "    \n",
    "    plot_and_play(np.concatenate(signals), sr)\n",
    "    shifted_signals = [pitch_correct(y=y, fn=f, sr=sr) for y, f in zip(signals, fn)]\n",
    "    \n",
    "    return shifted_signals\n",
    "\n",
    "def pitch_correct(y, fn, sr):\n",
    "    f0 = read_voice_frequency()\n",
    "    \n",
    "    n_steps = pitch_difference(fn=fn, f0=f0)\n",
    "\n",
    "    shifted_y = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=n_steps)\n",
    "\n",
    "    return shifted_y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_signals(signals, intervals):\n",
    "    stretchy = [stretch_signal(y=y, interval=interval) for y, interval in zip(signals, intervals)]\n",
    "    return stretchy\n",
    "\n",
    "def stretch_signal(y, interval):\n",
    "    note_duration = interval[1] - interval[0]\n",
    "    \n",
    "    rate = len(y) / float(note_duration)\n",
    "    stretched_y = librosa.effects.time_stretch(y, rate=rate)\n",
    "    \n",
    "    return stretched_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nussl\n",
    "\n",
    "def get_melody(y, sr):\n",
    "    signal = nussl.AudioSignal(audio_data_array=y, sample_rate=sr)\n",
    "    \n",
    "    melodia = nussl.separation.melodia.Melodia(signal)\n",
    "\n",
    "    melodia.run()\n",
    "\n",
    "    foreground_and_background = melodia.make_audio_signals()\n",
    "    \n",
    "    foreground = foreground_and_background[1]\n",
    "    \n",
    "    background = foreground_and_background[0]\n",
    "    \n",
    "    melody = foreground.to_mono()\n",
    "    \n",
    "    backtrack = background.to_mono()\n",
    "    \n",
    "    return melody, backtrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_length(lst1, lst2, lst3):\n",
    "    \n",
    "    length = min(len(lst1), len(lst2), len(lst3))\n",
    "    \n",
    "    return lst1[:length], lst2[:length], lst3[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_signals(song_signal, vocal_signal):\n",
    "    \n",
    "    min_length = min(len(song_signal), len(vocal_signal))\n",
    "    \n",
    "    song_normal = librosa.util.normalize(song_signal)\n",
    "    \n",
    "    vocals_normal = librosa.util.normalize(vocal_signal)\n",
    "    \n",
    "    both = song_normal[:min_length] + vocals_normal[:min_length]\n",
    "    \n",
    "    return both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_voice(intervals, signals, y):\n",
    "    padded_signals = [pad_signal(i, s, y) for i, s in zip(intervals, signals)]\n",
    "    voice_signal = np.sum(padded_signals, axis=0)\n",
    "    return voice_signal\n",
    "        \n",
    "def pad_signal(interval, signal, y):\n",
    "    start, end = interval\n",
    "    start_padding = np.zeros(start)\n",
    "    end_padding = np.zeros(len(y)-len(signal)-start)\n",
    "    padded_signal = np.concatenate([start_padding, signal, end_padding])\n",
    "    return padded_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2e(filename, text, sr):\n",
    "    song, sr = librosa.load(filename, sr=sr)\n",
    "    \n",
    "    melody, background = get_melody(song, sr)\n",
    "    \n",
    "    intervals, frequencies = onset_frequencies(melody, song, sr)\n",
    "    \n",
    "    syllables = get_syllable_signals(text, sr)\n",
    "    \n",
    "    syllables, intervals, frequencies = normalize_length(syllables, intervals, frequencies)\n",
    "    \n",
    "    pitch_corrected_syllables = autotune(syllables, fn=frequencies, sr=sr)\n",
    "    \n",
    "    time_corrected_syllables = stretch_signals(pitch_corrected_syllables, intervals)\n",
    "    \n",
    "    tuned_voice_signal = construct_voice(intervals, time_corrected_syllables, y=song)\n",
    "    \n",
    "    song_with_voice = overlay_signals(background, tuned_voice_signal)\n",
    "    \n",
    "    return song_with_voice    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 44100\n",
    "# f=open(\"twinkle_lyrics.txt\", \"r\")\n",
    "# if f.mode == 'r':\n",
    "#     text =f.read()\n",
    "\n",
    "    \n",
    "# text = 'happy birthday to you'\n",
    "y = e2e(filename='audio/twinkle.wav', text=text, sr=sr)\n",
    "\n",
    "plot_and_play(y, sr)\n",
    "\n",
    "# librosa.output.write_wav('redbone_output.wav', y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
