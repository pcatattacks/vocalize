{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython, numpy as np, scipy as sp, matplotlib.pyplot as plt, matplotlib, sklearn, librosa\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline\n",
    "\n",
    "import audio_utils\n",
    "import vamp\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"VAMP_PATH\"] = \"./melodia_plugins/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melodic_frequency_filepath(filepath):\n",
    "    audio, sr = librosa.load(filepath, mono=True)\n",
    "    data = vamp.collect(audio, sr, \"mtg-melodia:melodia\")\n",
    "    hop, melody = data['vector']\n",
    "    return melody\n",
    "\n",
    "def melodic_frequency(audio, sr):\n",
    "    data = vamp.collect(audio, sr, \"mtg-melodia:melodia\")\n",
    "    hop, melody = data['vector']\n",
    "    return melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A clearer option is to get rid of the negative values before plotting\n",
    "def plot_melody(melody):\n",
    "    timestamps = 8 * 128/44100.0 + np.arange(len(melody)) * (128/44100.0)\n",
    "    melody_pos = melody[:]\n",
    "    melody_pos[melody<=0] = None\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(timestamps, melody_pos)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAMP_CONSTANT = (128/44100.0)\n",
    "\n",
    "def timestamp_to_index(timestamp):\n",
    "    index = (timestamp - (8 * VAMP_CONSTANT)) / VAMP_CONSTANT\n",
    "    return int(index)\n",
    "\n",
    "def find_interval_freq(start_time, end_time, melody):\n",
    "    start_index = timestamp_to_index(start_time)\n",
    "    end_index = timestamp_to_index(end_time)\n",
    "    \n",
    "    relevant_frequencies = melody[start_index:end_index]\n",
    "    \n",
    "    freq = find_freq(relevant_frequencies)\n",
    "    \n",
    "    return freq\n",
    "\n",
    "def find_freq(melody):\n",
    "    voiced_frequencies = np.array([ freq for freq in melody if freq > 0 ])\n",
    "    freq = np.median(voiced_frequencies)\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_play(y, sr, label=''):\n",
    "    print(label)\n",
    "    IPython.display.display(Audio(y, rate=sr))\n",
    "    audio_utils.plot_audio(y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import subprocess\n",
    "\n",
    "SPEECH_FILEPATH = 'temp/tts_dump.wav'\n",
    "TEXT_FILEPATH = 'temp/text.txt'\n",
    "SCRIPT_FILEPATH = \"./text_gen.sh\"\n",
    "\n",
    "def tts(text):\n",
    "    \"\"\"\n",
    "        returns list of AudioSegments corresponding to each word in text\n",
    "    \"\"\"\n",
    "    tokens = text.split(' ')\n",
    "    \n",
    "    audio_chunks = [tts_word(token) for token in tokens]\n",
    "\n",
    "    return audio_chunks\n",
    "\n",
    "def tts_word(text):\n",
    "\n",
    "    with open(TEXT_FILEPATH, 'w+') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    p = subprocess.call(SCRIPT_FILEPATH, shell=True)\n",
    "\n",
    "    y = AudioSegment.from_wav(SPEECH_FILEPATH)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub.silence import split_on_silence\n",
    "\n",
    "def get_syllable_signals(text, sr):\n",
    "    params = {'min_silence_len': 40, \n",
    "              'silence_thresh': -18, \n",
    "              'keep_silence': 10\n",
    "             }\n",
    "    audio_words = tts_word(text)\n",
    "    audio_words = audio_words.set_frame_rate(sr)\n",
    "\n",
    "    audio_chunks = split_on_silence(audio_words, \n",
    "                                    min_silence_len=params['min_silence_len'], \n",
    "                                    silence_thresh=params['silence_thresh'], \n",
    "                                    keep_silence=params['keep_silence'])\n",
    "\n",
    "    all_segments = [np.array(chunk.get_array_of_samples(), dtype=float) for chunk in audio_chunks]\n",
    "\n",
    "    return all_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def onset_intervals(y, sr):\n",
    "#     onset_times = librosa.onset.onset_detect(y=y, sr=sr, units='time')\n",
    "    \n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr, aggregate=np.median)\n",
    "    times = librosa.frames_to_time(np.arange(len(onset_env)), sr=sr)\n",
    "    peaks = librosa.util.peak_pick(onset_env, pre_max=50, post_max=50, pre_avg=100, post_avg=100, delta=0.03, wait=60)\n",
    "    onset_times = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr, units='time',pre_max=50, post_max=50, pre_avg=100, post_avg=100, delta=0.03, wait=60)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(times, onset_env, alpha=0.8, label='Onset strength')\n",
    "    plt.vlines(times[peaks], 0, onset_env.max(), color='r', alpha=0.8, label='Selected peaks')\n",
    "    plt.legend(frameon=True, framealpha=0.8)\n",
    "    plt.axis('tight')\n",
    "    plt.tight_layout()\n",
    "    print(peaks)\n",
    "    print(onset_times.shape)\n",
    "    \n",
    "    final_time = len(y)/ sr\n",
    "    \n",
    "    intervals = [onset_interval(i, onset, onset_times, final_time) for i, onset in enumerate(onset_times)]\n",
    "    \n",
    "    return intervals\n",
    "\n",
    "def onset_interval(i, onset_start, onset_times, last_timestamp):\n",
    "    if i+1 >= len(onset_times):\n",
    "        return (onset_start, last_timestamp)\n",
    "\n",
    "    else:\n",
    "        return (onset_start, onset_times[i+1])\n",
    "    \n",
    "def onset_frequencies(y, sr):\n",
    "    intervals = onset_intervals(y, sr)\n",
    "    y_melody = melodic_frequency(y, sr)\n",
    "    y_freqs = [find_interval_freq(start, end, y_melody) for start, end in intervals]\n",
    "    return intervals, y_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_difference(fn, f0):\n",
    "    n_steps = np.round(np.log2(float(fn)/f0) * 12)\n",
    "    return n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_voice_frequency():\n",
    "    filepath = 'espeak-data/voices/robot'\n",
    "    with open(filepath) as f:\n",
    "        lines = [line.split(' ') for line in f.readlines()]\n",
    "        line = [line for line in lines if line and line[0] == 'pitch'][0]\n",
    "        f0 = int(line[1])\n",
    "        return f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def autotune(signals, fn, sr):\n",
    "    \n",
    "    shifted_signals = [pitch_correct(y=y, fn=f, sr=sr) for y, f in zip(signals, fn)]\n",
    "    \n",
    "    return shifted_signals\n",
    "\n",
    "def pitch_correct(y, fn, sr):\n",
    "    f0 = read_voice_frequency()\n",
    "    \n",
    "    n_steps = pitch_difference(fn=fn, f0=f0)\n",
    "\n",
    "    shifted_y = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=n_steps)\n",
    "\n",
    "    return shifted_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_signals(signals, intervals, sr):\n",
    "    stretchy = [stretch_signal(y=y, interval=interval, sr=sr) for y, interval in zip(signals, intervals)]\n",
    "    return stretchy\n",
    "\n",
    "def stretch_signal(y, interval, sr):\n",
    "    note_duration = interval[1] - interval[0]\n",
    "    y_duration = librosa.get_duration(y=y, sr=sr)\n",
    "    rate = y_duration/note_duration\n",
    "    stretched_y = librosa.effects.time_stretch(y, rate=rate)\n",
    "    return stretched_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nussl\n",
    "\n",
    "def get_melody(y, sr):\n",
    "    signal = nussl.AudioSignal(audio_data_array=y, sample_rate=sr)\n",
    "    \n",
    "    melodia = nussl.separation.melodia.Melodia(signal)\n",
    "\n",
    "    melodia.run()\n",
    "\n",
    "    foreground_and_background = melodia.make_audio_signals()\n",
    "\n",
    "    foreground = foreground_and_background[1]\n",
    "    background = foreground_and_background[0]\n",
    "    \n",
    "    backtrack = background.to_mono()\n",
    "    plot_and_play(backtrack, sr)\n",
    "    \n",
    "    melody = foreground.to_mono()\n",
    "    \n",
    "    return melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_length(lst1, lst2, lst3):\n",
    "    \n",
    "    length = min(len(lst1), len(lst2), len(lst3))\n",
    "    \n",
    "    return lst1[:length], lst2[:length], lst3[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_signals(song_signal, vocal_signal, sr):\n",
    "    \n",
    "    first_onset_sample = librosa.onset.onset_detect(y=song_signal, sr=sr, units='samples')[0]\n",
    "    \n",
    "    song_trimmed = song_signal[first_onset_sample:]\n",
    "\n",
    "    min_length = min(len(song_trimmed), len(vocal_signal))\n",
    "    \n",
    "    both = librosa.util.normalize(song_trimmed[:min_length]) + librosa.util.normalize(vocal_signal[:min_length])\n",
    "    \n",
    "    return both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2e(filename, text, sr):\n",
    "    song, sr = librosa.load(filename, sr=sr)\n",
    "    \n",
    "    melody = get_melody(song, sr)\n",
    "    \n",
    "    intervals, frequencies = onset_frequencies(melody, sr)\n",
    "    \n",
    "    print(len(intervals))\n",
    "    \n",
    "    for i, f in zip(intervals, frequencies):\n",
    "        print(i, f)\n",
    "    \n",
    "    syllables = get_syllable_signals(text, sr)\n",
    "    \n",
    "    syllables, intervals, frequencies = normalize_length(syllables, intervals, frequencies)\n",
    "    \n",
    "    pitch_corrected_syllables = autotune(syllables, fn=frequencies, sr=sr)\n",
    "    \n",
    "    time_corrected_syllables = stretch_signals(pitch_corrected_syllables, intervals, sr=sr)\n",
    "    \n",
    "    tuned_voice_signal = np.concatenate(time_corrected_syllables)\n",
    "    \n",
    "    song_with_voice = overlay_signals(song, tuned_voice_signal, sr)\n",
    "    \n",
    "    return song_with_voice\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 44100\n",
    "text = 'happy birthday to you'\n",
    "y = e2e(filename='audio/hbd_snip.wav', text=text, sr=sr)\n",
    "\n",
    "\n",
    "plot_and_play(y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
